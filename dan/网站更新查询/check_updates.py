# main_test.py
# ç‰ˆæœ¬ V20-test-auto-driver: è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½åŒçº§ç›®å½•ä¸‹çš„é©±åŠ¨ç¨‹åº

import pandas as pd
import os
import sys  # <-- æ–°å¢ï¼šå¯¼å…¥sysæ¨¡å—ä»¥æ£€æµ‹æ“ä½œç³»ç»Ÿ
import requests
import re
import time
import random
from datetime import datetime, date
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from dateutil.parser import parse as parse_date
from dateutil.parser import ParserError
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
import urllib3
import logging

# ... [ä» CONFIGURATION åˆ° generate_html_report çš„æ‰€æœ‰ä»£ç ä¸ä¸Šä¸€ç‰ˆå®Œå…¨ç›¸åŒï¼Œè¿™é‡Œçœç•¥] ...
# ... æ‚¨åªéœ€å¤åˆ¶å¹¶æ›¿æ¢ä¸‹é¢çš„ main å‡½æ•°å³å¯ ...

# ==============================================================================
# 1. æœ¬åœ°æµ‹è¯•é…ç½®
# ==============================================================================
EXCEL_FILE_PATH = "å®‰å¾½çœç½‘å€.xlsx"
OUTPUT_DIR = "reports"
TARGET_DATE_STR = '2025-08-16'
KEY_KEYWORDS_STR = "æ‹›è˜, äººæ‰å¼•è¿›"
EXCLUDE_KEYWORDS_STR = "åšå£«, æ‹Ÿè˜ç”¨, é«˜å±‚æ¬¡, å…¬ç¤º"

# ==============================================================================
# 2. NETWORKING & PARSING SETUP (ä¸ V20 GUI ç‰ˆæœ¬å®Œå…¨ä¸€è‡´)
# ==============================================================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
from requests.adapters import HTTPAdapter

try:
    from requests.packages.urllib3.util.ssl_ import create_urllib3_context
except ImportError:
    from urllib3.util.ssl_ import create_urllib3_context
CIPHERS = (
    'ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384')


class Tls12Adapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        self.poolmanager = requests.packages.urllib3.PoolManager(num_pools=connections, maxsize=maxsize, block=block,
                                                                 ssl_context=create_urllib3_context(ciphers=CIPHERS))


DATE_REGEX = re.compile(
    r'\[?(\d{4}[-å¹´/\.]\s*\d{1,2}[-æœˆ/\.]\s*\d{1,2})\]?|(\d{1,2}\s*[A-Za-z]{3,}\s*,?\s*\d{4})|([A-Za-z]{3,}\s*\d{1,2},?\s*\d{4})|\[?(\d{1,2}[-æœˆ/\.]\d{1,2})\]?')


# ==============================================================================
# 3. CORE FUNCTIONS (ä¸ V20 GUI ç‰ˆæœ¬å®Œå…¨ä¸€è‡´)
# ==============================================================================

def handle_yearless_date(date_str: str) -> str:
    try:
        date_str_normalized = date_str.replace('æœˆ', '-').replace('æ—¥', '').strip('[]/')
        if re.match(r'^\d{4}[-/]\d{1,2}$', date_str_normalized): date_str_normalized += '-01'
        parsed_date = parse_date(date_str_normalized).date()
        today = date.today()
        if parsed_date.year == today.year and (parsed_date - today).days > 60: return parsed_date.replace(
            year=today.year - 1).strftime('%Y-%m-%d')
        return parsed_date.strftime('%Y-%m-%d')
    except (ParserError, ValueError):
        today = date.today()
        full_date_str = f"{today.year}-{date_str_normalized}"
        try:
            parsed_date = parse_date(full_date_str).date()
            if (parsed_date - today).days > 60: return parsed_date.replace(year=today.year - 1).strftime('%Y-%m-%d')
            return parsed_date.strftime('%Y-%m-%d')
        except (ParserError, ValueError):
            return None


def parse_html_for_articles(html_content: str, base_url: str, target_date: datetime.date,
                            key_keywords: list, exclude_keywords: list):
    soup = BeautifulSoup(html_content, 'lxml')
    key_updates, other_updates = [], []
    processed_urls = set()

    def process_article(title, link_href, date_str):
        if any(keyword in title for keyword in exclude_keywords if keyword): return
        final_date = None
        try:
            clean_date_str = date_str.strip('[]')
            if re.search(r'\d{4}', clean_date_str):
                final_date = parse_date(clean_date_str).date()
            else:
                processed_date_str = handle_yearless_date(clean_date_str)
                if processed_date_str: final_date = datetime.strptime(processed_date_str, '%Y-%m-%d').date()
            if final_date and final_date >= target_date:
                absolute_url = urljoin(base_url, link_href)
                if absolute_url in processed_urls: return
                processed_urls.add(absolute_url)
                article_data = {'title': title, 'date': final_date.strftime('%Y-%m-%d'), 'url': absolute_url}
                if any(keyword in title for keyword in key_keywords if keyword):
                    key_updates.append(article_data)
                else:
                    other_updates.append(article_data)
        except (ParserError, ValueError):
            pass

    main_content_area = soup.find('div', class_=re.compile(r'PicList|content2|zpgw_box|list_box', re.I))
    search_scope = main_content_area if main_content_area else soup
    containers = search_scope.find_all(['li', 'tr', 'dd', 'article'])
    if not containers: containers = search_scope.find_all('div',
                                                          class_=re.compile(r'item|post|list|news|row|col', re.I))
    for item in containers:
        link_tag = item.find('a', href=True)
        if not link_tag: continue
        raw_title_text = link_tag.get_text(strip=True)
        title = raw_title_text.split('æ¥æºï¼š')[0].split('æ—¶é—´ï¼š')[0].strip()
        title_tag = link_tag.find(['h1', 'h2', 'h3', 'h4', 'div', 'p'],
                                  class_=re.compile(r'title|list|name|text', re.I))
        title = title_tag.get_text(strip=True) if title_tag and title_tag.get_text(strip=True) else title
        if len(title.split()) < 2 and len(title) < 8: continue
        date_str = None
        date_tag = item.find('time')
        if date_tag:
            date_str = date_tag.get_text(strip=True)
        else:
            time_div = item.find(class_=re.compile(r'time|date', re.I))
            if time_div:
                match = DATE_REGEX.search(' '.join(time_div.stripped_strings).replace('/', '-'))
                if match: date_str = match.group(0)
        if not date_str:
            match = DATE_REGEX.search(item.get_text(separator=' ', strip=True))
            if match: date_str = match.group(0)
        if date_str: process_article(title, link_tag['href'], date_str)
    return {'status': 'success', 'key_updates': key_updates, 'other_updates': other_updates}


def find_updates_dynamic_selenium(driver, base_url: str, target_date: datetime.date, key_keywords: list,
                                  exclude_keywords: list):
    try:
        driver.get(base_url)
        WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(random.uniform(2, 4))
        try:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
            time.sleep(random.uniform(1, 2))
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(1, 2))
        except Exception:
            pass
        html_content = driver.page_source
        initial_parse_result = parse_html_for_articles(html_content, base_url, target_date, key_keywords,
                                                       exclude_keywords)
        if not initial_parse_result['key_updates'] and not initial_parse_result['other_updates'] and len(
            html_content) < 20000:
            iframes = driver.find_elements(By.TAG_NAME, 'iframe')
            if iframes:
                try:
                    driver.switch_to.frame(iframes[0])
                    time.sleep(2)
                    html_content = driver.page_source
                    driver.switch_to.default_content()
                except Exception:
                    pass
        return parse_html_for_articles(html_content, base_url, target_date, key_keywords, exclude_keywords)
    except TimeoutException:
        html_content = driver.page_source if driver else ""
        if len(html_content) < 500: return {'status': 'error', 'reason': "Seleniumé”™è¯¯: é¡µé¢åŠ è½½è¶…æ—¶ä¸”å†…å®¹ä¸ºç©º"}
        return parse_html_for_articles(html_content, base_url, target_date, key_keywords, exclude_keywords)
    except Exception as e:
        return {'status': 'error', 'reason': f"Seleniumé”™è¯¯: {type(e).__name__}: {str(e)}".strip()}


def find_updates_static(base_url: str, target_date: datetime.date, key_keywords: list, exclude_keywords: list):
    session = requests.Session()
    session.mount('https://', Tls12Adapter())
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
        response = session.get(base_url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        return parse_html_for_articles(response.text, base_url, target_date, key_keywords, exclude_keywords)
    except requests.exceptions.RequestException as e:
        return {'status': 'error', 'reason': f"ç½‘ç»œé”™è¯¯: {type(e).__name__}"}


def load_urls_from_excel(file_path):
    try:
        df = pd.read_excel(file_path, header=0)
        return [(row.iloc[0], row.iloc[1]) for index, row in df.iterrows()]
    except FileNotFoundError:
        print(f"é”™è¯¯: Excelæ–‡ä»¶æœªæ‰¾åˆ° '{file_path}'ã€‚")
        return None
    except Exception as e:
        print(f"é”™è¯¯: è¯»å–Excelå¤±è´¥: {e}")
        return None


def generate_html_report(updated_sites, no_update_sites, error_sites, target_date_str, output_dir):
    if not os.path.exists(output_dir): os.makedirs(output_dir)
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    report_filename = os.path.join(output_dir, f"ç½‘é¡µæ›´æ–°æŠ¥å‘Š-{datetime.now().strftime('%Y%m%d-%H%M%S')}.html")
    key_sites_html, other_sites_html = [], []
    for site in updated_sites:
        if site['key_updates']: key_sites_html.append(
            f'''<div class="site-block"><div class="site-title">{site['name']}</div><div class="site-url"><a href="{site['url']}" target="_blank">{site['url']}</a></div><ul>{''.join([f'<li class="key-update-item"><span class="date">[{update["date"]}]</span> <a href="{update["url"]}" target="_blank">{update["title"]}</a></li>' for update in site["key_updates"]])}</ul></div>''')
        if site['other_updates']: other_sites_html.append(
            f'''<div class="site-block"><div class="site-title">{site['name']}</div><div class="site-url"><a href="{site['url']}" target="_blank">{site['url']}</a></div><ul>{''.join([f'<li class="update-item"><span class="date">[{update["date"]}]</span> <a href="{update["url"]}" target="_blank">{update["title"]}</a></li>' for update in site["other_updates"]])}</ul></div>''')
    html_template = f"""
    <!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><title>ç½‘é¡µæ›´æ–°æ£€æŸ¥æŠ¥å‘Š</title><style>body{{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI","Microsoft YaHei",sans-serif;margin:0 auto;max-width:1000px;padding:20px;color:#333}}h1,h2{{color:#1a73e8;border-bottom:2px solid #e0e0e0;padding-bottom:10px}}h2.key-title{{color:#ff8f00}}h2.other-title{{color:#1e8e3e}}.summary{{background-color:#f8f9fa;border-left:5px solid #1a73e8;padding:15px;margin:20px 0}}.site-block{{margin-bottom:25px;padding:15px;border:1px solid #ddd;border-radius:8px;box-shadow:0 2px 4px rgba(0,0,0,0.05)}}.site-title{{font-size:1.2em;font-weight:bold;color:#202124}}.site-url{{font-size:0.9em;color:#5f6368;word-break:break-all}}ul{{list-style-type:none;padding-left:0}}li.update-item{{margin-top:10px;padding:10px;background-color:#f1f8e9;border-radius:5px}}li.key-update-item{{margin-top:10px;padding:10px;background-color:#fff8e1;border-left:3px solid #ff8f00;border-radius:5px}}li.no-update-item,li.error-item{{margin-top:5px;padding:5px;background-color:#f3f3f3;border-radius:5px}}.date{{font-weight:bold;color:#1e8e3e}}.error-reason{{color:#d93025;font-style:italic}}a{{color:#1a73e8;text-decoration:none}}a:hover{{text-decoration:underline}}</style></head><body>
    <h1>ç½‘é¡µæ›´æ–°æ£€æŸ¥æŠ¥å‘Š</h1><div class="summary"><strong>æŠ¥å‘Šç”Ÿæˆæ—¶é—´:</strong> {now}<br><strong>ç›‘æ§èµ·å§‹æ—¥æœŸ:</strong> {target_date_str} ä¹‹å<br><strong>ç»“æœæ¦‚è¦:</strong> <span style="color:#1e8e3e;">{len(updated_sites)}</span> ä¸ªç½‘ç«™æœ‰æ›´æ–° | <span style="color:#5f6368;">{len(no_update_sites)}</span> ä¸ªæ— æ›´æ–° | <span style="color:#d93025;">{len(error_sites)}</span> ä¸ªè®¿é—®å¤±è´¥</div>
    <h2 class="key-title">â­ é‡ç‚¹å…³æ³¨æ›´æ–°</h2>{''.join(key_sites_html) if key_sites_html else "<p>æœ¬æ¬¡æ²¡æœ‰æ£€æµ‹åˆ°ç›¸å…³çš„é‡ç‚¹æ›´æ–°ã€‚</p>"}
    <h2 class="other-title">ğŸ“„ å…¶ä»–æ›´æ–°</h2>{''.join(other_sites_html) if other_sites_html else "<p>æœ¬æ¬¡æ²¡æœ‰æ£€æµ‹åˆ°å…¶ä»–ç±»å‹çš„æ›´æ–°ã€‚</p>"}
    <h2>â„¹ï¸ æ— æ›´æ–°çš„ç½‘ç«™</h2><ul>{''.join([f'<li class="no-update-item"><span class="site-title">{site["name"]}</span> - <a href="{site["url"]}" target="_blank">{site["url"]}</a></li>' for site in no_update_sites]) if no_update_sites else "<p>æ‰€æœ‰ç½‘ç«™å‡æœ‰æ›´æ–°æˆ–è®¿é—®å¤±è´¥ã€‚</p>"}</ul>
    <h2>âŒ æ— æ³•è®¿é—®çš„ç½‘ç«™</h2><ul>{''.join([f'<li class="error-item"><span class="site-title">{site["name"]}</span> - {site["url"]}<br><span class="error-reason">åŸå› : {site["reason"]}</span></li>' for site in error_sites]) if error_sites else "<p>æ‰€æœ‰ç½‘ç«™å‡å¯æ­£å¸¸è®¿é—®ã€‚</p>"}</ul>
    </body></html>"""
    try:
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(html_template)
        print(f"\næŠ¥å‘Šç”ŸæˆæˆåŠŸï¼å·²ä¿å­˜è‡³: {report_filename}")
    except Exception as e:
        print(f"\né”™è¯¯: å†™å…¥æŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {e}")


# ==============================================================================
# 4. MAIN EXECUTION LOGIC (MODIFIED FOR AUTO DRIVER DETECTION)
# ==============================================================================
def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    print("--- ç½‘é¡µæ›´æ–°æ£€æŸ¥è„šæœ¬ (V20-test-auto-driver) ---")
    key_list = [k.strip() for k in KEY_KEYWORDS_STR.split(',') if k.strip()]
    exclude_list = [k.strip() for k in EXCLUDE_KEYWORDS_STR.split(',') if k.strip()]

    print(f"å…³é”®è¯: {key_list if key_list else 'æ— '}")
    print(f"æ’é™¤è¯: {exclude_list if exclude_list else 'æ— '}")

    sites_to_check = load_urls_from_excel(EXCEL_FILE_PATH)
    if not sites_to_check: return
    try:
        target_date = datetime.strptime(TARGET_DATE_STR, '%Y-%m-%d').date()
    except ValueError:
        print(f"é”™è¯¯: æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·ä½¿ç”¨ 'YYYY-MM-DD'ã€‚")
        return

    print(f"ç›®æ ‡æ—¥æœŸ: >= {TARGET_DATE_STR}\nå¾…æ£€æŸ¥ç½‘ç«™: {len(sites_to_check)}\n")
    updated_sites, no_update_sites, error_sites = [], [], []

    dynamic_driver = None
    try:
        total_sites = len(sites_to_check)
        for i, (name, url) in enumerate(sites_to_check):
            print(f"[{i + 1}/{total_sites}] æ£€æŸ¥: {name} (å¿«é€Ÿæ¨¡å¼)...")
            result = find_updates_static(url, target_date, key_list, exclude_list)

            is_static_empty = (
                    result['status'] == 'success' and not result['key_updates'] and not result['other_updates'])
            if result['status'] == 'error' or is_static_empty:
                reason = result.get('reason', 'æœªå‘ç°æ›´æ–°')
                print(f"  -> å¿«é€Ÿæ¨¡å¼å¤±è´¥({reason})ï¼Œå°è¯•åŠ¨æ€æ¨¡å¼...")

                if dynamic_driver is None:
                    print("  -> [LOG] å‡†å¤‡åˆå§‹åŒ–åŠ¨æ€æµè§ˆå™¨å®ä¾‹ (åªéœ€ä¸€æ¬¡)...")

                    # --- æ ¸å¿ƒä¿®æ”¹ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶æ„å»ºé©±åŠ¨è·¯å¾„ ---
                    driver_path = None
                    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½•
                    base_path = os.path.dirname(os.path.abspath(__file__))

                    if sys.platform.startswith('win'):
                        # Windowsç³»ç»Ÿ
                        potential_path = os.path.join(base_path, 'chromedriver.exe')
                        if os.path.exists(potential_path):
                            driver_path = potential_path
                    elif sys.platform.startswith('darwin') or sys.platform.startswith('linux'):
                        # macOS æˆ– Linux ç³»ç»Ÿ
                        potential_path = os.path.join(base_path, 'chromedriver')
                        if os.path.exists(potential_path):
                            driver_path = potential_path

                    # --- åˆå§‹åŒ–æµè§ˆå™¨ ---
                    options = uc.ChromeOptions()
                    options.add_argument('--headless=new')
                    # ... å…¶ä»– options ...

                    try:
                        print("  -> [LOG] å‡†å¤‡è°ƒç”¨ uc.Chrome()...")
                        if driver_path:
                            print(f"  -> [LOG] å‘ç°å¹¶ä½¿ç”¨åŒçº§ç›®å½•ä¸‹çš„é©±åŠ¨: {driver_path}")
                            dynamic_driver = uc.Chrome(options=options, driver_executable_path=driver_path)
                        else:
                            print("  -> [LOG] æœªåœ¨åŒçº§ç›®å½•å‘ç°é©±åŠ¨ï¼Œå°†å°è¯•è‡ªåŠ¨ä¸‹è½½å’Œç®¡ç†...")
                            dynamic_driver = uc.Chrome(options=options)

                        print("  -> [LOG] uc.Chrome() è°ƒç”¨æˆåŠŸï¼æµè§ˆå™¨å®ä¾‹å·²åˆ›å»ºã€‚")
                        dynamic_driver.set_page_load_timeout(30)
                        print("  -> åŠ¨æ€æµè§ˆå™¨å·²å¯åŠ¨ã€‚")
                    except Exception as e:
                        print("\n\n" + "=" * 20 + " åˆå§‹åŒ–æµè§ˆå™¨æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯! " + "=" * 20)
                        print(f"  -> [ERROR] é”™è¯¯ç±»å‹: {type(e).__name__}")
                        print(f"  -> [ERROR] é”™è¯¯ä¿¡æ¯: {e}")
                        print("=" * 65 + "\n")
                        break

                if not dynamic_driver:
                    print("  -> åŠ¨æ€æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡åç»­æ‰€æœ‰åŠ¨æ€æ£€æŸ¥ã€‚")
                    error_sites.append({'name': name, 'url': url, 'reason': "åŠ¨æ€æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥"})
                    continue

                result = find_updates_dynamic_selenium(dynamic_driver, url, target_date, key_list, exclude_list)

            if result['status'] == 'success':
                key_updates, other_updates = result.get('key_updates', []), result.get('other_updates', [])
                if key_updates or other_updates:
                    sorted_key = sorted(key_updates, key=lambda x: x['date'], reverse=True)
                    sorted_other = sorted(other_updates, key=lambda x: x['date'], reverse=True)
                    updated_sites.append(
                        {'name': name, 'url': url, 'key_updates': sorted_key, 'other_updates': sorted_other})
                    print(f"  -> âœ… å‘ç° {len(sorted_key)} æ¡é‡ç‚¹æ›´æ–°, {len(sorted_other)} æ¡å…¶ä»–æ›´æ–°ï¼")
                else:
                    no_update_sites.append({'name': name, 'url': url})
                    print("  -> â„¹ï¸ æœªå‘ç°æœ‰æ•ˆæ›´æ–°ã€‚")
            else:
                error_sites.append({'name': name, 'url': url, 'reason': result['reason']})
                print(f"  -> âŒ è®¿é—®å¤±è´¥: {result['reason']}")

            if i < total_sites - 1:
                sleep_time = random.uniform(2, 5)
                print(f"--- (å»¶æ—¶ {sleep_time:.1f} ç§’) ---\n")
                time.sleep(sleep_time)

    finally:
        if dynamic_driver:
            print("\næ­£åœ¨å…³é—­åŠ¨æ€æµè§ˆå™¨å®ä¾‹...")
            dynamic_driver.quit()
            print("æµè§ˆå™¨å·²å…³é—­ã€‚")

    generate_html_report(updated_sites, no_update_sites, error_sites, TARGET_DATE_STR, OUTPUT_DIR)


if __name__ == "__main__":
    main()